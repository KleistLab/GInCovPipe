# Main workflow of GInPipe
import os
from pathlib import Path

#filename without extension
basefilename = Path(config["samples"]).stem
filetype = Path(config["samples"]).suffix

# Check if reported cases are given (if not, pipeline will terminate earlier)
rep_cases = config["reported_cases"]
rc = True

if len(rep_cases) != 5 or not rep_cases[0]:
	rep_cases = [""] * 5
	rc = False

#rc = Path(get_reported_cases_params()[0]).exists()

report: "report/workflow.rst"

snv_file = config["samples"]

# If not particular name is given, use the sample files name
name = config["name"]
if not name:
	name = basefilename

def get_input():
	# if reported cases are given, the minimal incidence can be obtained, else 
	if rc:
		return "results/incidence/minimal_incidence_"+name+".csv"
	# else the smoothed incidence correlate
	return "results/phi_estimates/smoothed_phi_estimates_" + name + ".csv"



rule all:
    input:
        get_input()

#### Optional preprocessing of sequences files in fasta format
if filetype == '.fasta':
	# If no SNV file is given, it need to be created
	snv_file = "results/snv/" + name + ".csv"

	rule strip_whitespaces:
		input:
			config["samples"]
		output:
			temp(expand("results/raw/{sample}_fixed1.fasta", sample=name))
		conda:
			"env/env.yml"
		log:
			expand("logs/ws_{sample}.log", sample=name)
		shell:
			"(reformat.sh in={input} out={output} underscore ignorejunk overwrite=true) 2> {log}"

	rule samtools_faidx:
		input:
			expand("{reference}", reference=config["reference"])
		output:
			temp(expand("{reference}.fai", reference=config["reference"]))
		log:
			expand("logs/faidx_{sample}.log",sample=basefilename)
		shell:
			"(samtools faidx {input}) 2> {log}"

	rule replace_dashes:
		input:
			"results/raw/{sample}_fixed1.fasta"
		output:
			temp("results/raw/{sample}_fixed12.fasta")
		conda:
			"env/env.yml"
		log:
			"logs/dashes_{sample}.log"
		shell:
			"(seqkit replace -s -p '-' -r 'N' {input} -o {output}) 2> {log}"

	rule samtools_dict:
		input:
			expand("{reference}", reference=config["reference"])
		output:
			temp(expand("{reference}.dict", reference=config["reference"][:-6]))
		conda:
			"env/env.yml"
		log:
			expand("logs/samtools_dict_{reference}.log",reference=config["reference"])
		shell:
			"(samtools dict {input} -o {output}) 2> {log}"

	rule minimap_index_ref:
		input:
			expand("{reference}", reference=config["reference"])
		output:
			temp(expand("{reference}.mmi", reference=config["reference"][:-6]))
		conda:
			"env/env.yml"
		log:
			expand("logs/minimap_index_{reference}.log",reference=config["reference"])
		shell:
			"(minimap2 -d {output} {input}) 2> {log}"

	rule minimap:
		input:
			ref = expand("{reference}.mmi", reference=config["reference"][:-6]),
			s = "results/raw/{sample}_fixed12.fasta"
		output:
			temp("results/bam/{sample}.bam")
		conda:
			"env/env.yml"
		log:
			"logs/map_{sample}.log",
		shell:
			"(minimap2 -a --eqx {input.ref} {input.s} | samtools view -Sb -F 0x900 > {output}) 2> {log}"

	rule flagstats:
		input:
			"results/bam/{sample}.bam"
		output:
			temp("results/bam/{sample}_flagstat.txt")
		conda:
			"env/env.yml"
		log:
			"logs/flagstat_{sample}.log"
		shell:
			"samtools flagstat {input} > {output} 2> {log}"

	rule sort_bam:
		input:
			bam = "results/bam/{sample}.bam",
			stat = "results/bam/{sample}_flagstat.txt"
		output:
			temp("results/bam/{sample}-sorted.bam")
		log:
			"logs/sort_{sample}.log"
		conda:
			"env/env.yml"
		shell:
			"(samtools sort {input.bam} > {output}) 2> {log}"

	rule index_bam:
		input:
			"results/bam/{sample}-sorted.bam"
		output:
			temp("results/bam/{sample}-sorted.bam.bai")
		conda:
			"env/env.yml"
		log:
			"logs/index_{sample}.log"
		shell:
			"(samtools index {input}) 2> {log}"
	
	rule get_snvs:
		input:
			bam="results/bam/"+name+"-sorted.bam",
			bai="results/bam/"+name+"-sorted.bam.bai",
			ref=config["reference"]
		output:
			snv = snv_file
		script:
			"scripts/bam_to_snv.py"



##### From here, the call remains the same, calling it with SNV table (covSonar like)

rule run_incidence_estimation:
	input:
		snv_file = snv_file,
	output:
		phi_estimates = "results/phi_estimates/phi_estimates_per_bin_" + name + ".csv",
		seq_info = "results/phi_estimates/sequence_stats_per_day_" + name + ".csv",
	params:
		seq_per_bin = config["seq_per_bin"],
		days_per_bin = config["days_per_bin"],
		cutoff = config["freq_cutoff"],
		name = name,
		masked_positions = config["masking"]
	conda:
		"env/env.yml"
	script:
		"scripts/run_incidence_estimation.py"

rule smoothing:
	input:
		phi_table = "results/phi_estimates/phi_estimates_per_bin_" + name + ".csv"
	params:
		min_bin_size = config["min_bin_size"],
		min_days_span = config["min_days_span"],
		max_days_span = config["max_days_span"],
		smoothing_bandwidth = config["smoothing_bandwidth_phi"],
		name = name
	conda:
		"env/env.yml"
	output:
		smoothed_phi = "results/phi_estimates/smoothed_phi_estimates_" + name + ".csv"
	script:
			"scripts/smooth_phi_estimates.py"

if rc:
	rule minimal_incidence:
		input:
			smoothed_phi = "results/phi_estimates/smoothed_phi_estimates_" + name + ".csv"
		params:
			rep_cases = rep_cases[0],
			sep = rep_cases[1],
			col_date = rep_cases[2],
			col_case = rep_cases[3],
			date_format = rep_cases[4],
			smoothing_bandwidth = config["smoothing_bandwidth_mi"],
			from_date = config["from_date"],
			to_date = config["to_date"],
			name = name
		conda:
			"env/env.yml"
		output:
			mi_table = "results/incidence/minimal_incidence_" + name + ".csv"
		script:
			"scripts/calculate_case_ascertainment.py"

# Main workflow of GInPipe
import os
from pathlib import Path

# def get_reported_cases_params():
#     rep_cases = config["reported_cases"]
#     if not :
#         rep_cases = [""] * 5
#     return rep_cases

#filename without extension
basefilename = Path(config["samples"]).stem
filetype = Path(config["samples"]).suffix

# Check if reported cases are given (if not, pipeline will terminate earlier)
rep_cases = config["reported_cases"]
rc = True
print(rep_cases)
if len(rep_cases) != 5 or not rep_cases[0]:
	rep_cases = [""] * 5
	rc = False

#rc = Path(get_reported_cases_params()[0]).exists()

report: "report/workflow.rst"

snv_file = config["samples"]

# If not particular name is given, use the sample files name
name = config["name"]
if not name:
	name = basefilename

def get_input():
	# if reported cases are given, the minimal incidence can be obtained, else 
	if rc:
		return "results/incidence/minimal_incidence_"+name+".csv"
	# else the smoothed incidence correlate
	return "results/phi_estimates/smoothed_phi_estimates_" + name + ".csv"



rule all:
    input:
        get_input()

#### Optional preprocessing of sequences files in fasta format
if filetype == '.fasta':
	
	# If no SNV file is given, it need to be created
	snv_file = "results/snv/" + name + ".csv"

	rule strip_whitespaces:
		input:
			config["samples"]
		output:
			expand("results/raw/{sample}_fixed1.fasta", sample=basefilename)
		conda:
			"env/env.yml"
		log:
			expand("logs/ws_{sample}.log", sample=basefilename)
		shell:
			"(reformat.sh in={input} out={output} underscore ignorejunk overwrite=true) 2> {log}"

	rule samtools_faidx:
		input:
			expand("{reference}", reference=config["consensus"])
		output:
			expand("{reference}.fai", reference=config["consensus"])
		log:
			expand("logs/faidx_{sample}.log",sample=basefilename)
		shell:
			"(samtools faidx {input}) 2> {log}"

	rule replace_dashes:
		input:
			"results/raw/{sample}_fixed1.fasta"
		output:
			"results/raw/{sample}_fixed12.fasta"
		conda:
			"env/env.yml"
		log:
			"logs/dashes_{sample}.log"
		shell:
			"(seqkit replace -s -p '-' -r 'N' {input} -o {output}) 2> {log}"

	rule samtools_dict:
		input:
			expand("{reference}", reference=config["consensus"])
		output:
			expand("{reference}.dict", reference=config["consensus"][:-6])
		conda:
			"env/env.yml"
		log:
			expand("logs/samtools_dict_{reference}.log",reference=config["consensus"])
		shell:
			"(samtools dict {input} -o {output}) 2> {log}"

	rule minimap_index_ref:
		input:
			expand("{reference}", reference=config["consensus"])
		output:
			expand("{reference}.mmi", reference=config["consensus"][:-6])
		conda:
			"env/env.yml"
		log:
			expand("logs/minimap_index_{reference}.log",reference=config["consensus"])
		shell:
			"(minimap2 -d {output} {input}) 2> {log}"

	rule minimap:
		input:
			ref = expand("{reference}.mmi", reference=config["consensus"][:-6]),
			s = "results/raw/{sample}_fixed12.fasta"
		output:
			"results/bam/{sample}.bam"
		conda:
			"env/env.yml"
		log:
			"logs/map_{sample}.log",
		shell:
			"(minimap2 -a --eqx {input.ref} {input.s} | samtools view -Sb -F 0x900 > {output}) 2> {log}"

	rule flagstats:
		input:
			"results/bam/{sample}.bam"
		output:
			"results/bam/{sample}_flagstat.txt"
		conda:
			"env/env.yml"
		log:
			"logs/flagstat_{sample}.log"
		shell:
			"samtools flagstat {input} > {output} 2> {log}"

	rule sort_bam:
		input:
			bam = "results/bam/{sample}.bam",
			stat = "results/bam/{sample}_flagstat.txt"
		output:
			"results/bam/{sample}-sorted.bam"
		log:
			"logs/sort_{sample}.log"
		conda:
			"env/env.yml"
		shell:
			"(samtools sort {input.bam} > {output}) 2> {log}"

	rule index_bam:
		input:
			"results/bam/{sample}-sorted.bam"
		output:
			"results/bam/{sample}-sorted.bam.bai"
		conda:
			"env/env.yml"
		log:
			"logs/index_{sample}.log"
		shell:
			"(samtools index {input}) 2> {log}"

	rule samtools_idxstats:
		input:
			"results/bam/{sample}-sorted.bam"
		output:
			"results/bam/{sample}-sorted-stats.txt"
		conda:
			"env/env.yml"
		log:
			"logs/idxstats_{sample}.log"
		shell:
			"(samtools idxstats {input} > {output}) 2> {log}"

	rule bam_to_snv:
		input:
			bam="results/bam/{sample}-sorted.bam"
		output:
			snv=snv_file



##### From here, the call remains the same, calling it with SNV table (covSonar like)
print(snv_file)

rule run_incidence_estimation:
	input:
		snv_file = snv_file,
	output:
		phi_estimates = "results/phi_estimates/phi_estimates_per_bin_" + name + ".csv",
		seq_info = "results/phi_estimates/sequence_stats_per_day_" + name + ".csv",
	params:
		seq_per_bin = config["seq_per_bin"],
		days_per_bin = config["days_per_bin"],
		cutoff = config["freq_cutoff"],
		name = name,
		masked_positions = config["masking"],
		vcf_file = config["vcf_file"]
	conda:
		"env/env.yml"
	log:
		"logs/binning_and_estimation.log"
	script:
		"scripts/run_incidence_estimation.py"

rule smoothing:
	input:
		phi_table = "results/phi_estimates/phi_estimates_per_bin_" + name + ".csv"
	params:
		min_bin_size = config["min_bin_size"],
		min_days_span = config["min_days_span"],
		max_days_span = config["max_days_span"],
		smoothing_window = 7
	conda:
		"env/env.yml"
	log:
		"logs/smoothing.log"
	output:
		smoothed_phi = "results/phi_estimates/smoothed_phi_estimates_" + name + ".csv"
	script:
			"scripts/smooth_phi_estimates.py"


# shell:
# 	"Rscript {workflow.basedir}/scripts/incidence/computeSmoothValues.R {input.infile} \
# 		{params.group} {output.result} {params.rep_cases[0]} '{params.rep_cases[1]}' \
# 		{params.rep_cases[2]} {params.rep_cases[3]} {params.rep_cases[4]}"

if rc:
	rule minimal_incidence:
		input:
			smoothed_phi = "results/phi_estimates/smoothed_phi_estimates_" + name + ".csv"
		params:
			rep_cases = rep_cases[0],
			sep = rep_cases[1],
			col_date = rep_cases[2],
			col_case = rep_cases[3],
			date_format = rep_cases[4],
			smoothing_window_phi = config["smoothing_window_phi"],
			smoothing_window_rep = config["smoothing_window_rep"],
			from_date = config["from_date"],
			to_date = config["to_date"]
		conda:
			"env/env.yml"
		log:
			"logs/minimal_incidence.log"
		output:
			mi_table = "results/incidence/minimal_incidence_" + name + ".csv"
		script:
			"scripts/calculate_case_ascertainment.py"
